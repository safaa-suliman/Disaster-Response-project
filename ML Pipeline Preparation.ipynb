{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\okekec\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\okekec\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\okekec\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    " #import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from sqlalchemy import create_engine\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger'])\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///Project3.db')\n",
    "df = pd.read_sql(\"SELECT * FROM df\", engine)\n",
    "X = df['message']\n",
    "y = df.iloc[:, 4:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # Convert text to lowercase and remove punctuation\n",
    "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using MultiOutputClassifier to build an ML pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "           n_jobs=1))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training the model with a random seed of 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 1) \n",
    "np.random.seed(42)\n",
    "pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_metrics(actual, predicted, col_names):\n",
    "    \"\"\"Calculate evaluation metrics for ML model\n",
    "    \n",
    "    Args:\n",
    "    actual: array. Array containing actual labels.\n",
    "    predicted: array. Array containing predicted labels.\n",
    "    col_names: list of strings. List containing names for each of the predicted fields.\n",
    "       \n",
    "    Returns:\n",
    "    metrics_df: dataframe. Dataframe containing the accuracy, precision, recall \n",
    "    and f1 score for a given set of actual and predicted labels.\n",
    "    \"\"\"\n",
    "    metrics = []\n",
    "    \n",
    "    # Calculate evaluation metrics for each set of labels\n",
    "    for i in range(len(col_names)):\n",
    "        accuracy = accuracy_score(actual[:, i], predicted[:, i])\n",
    "        precision = precision_score(actual[:, i], predicted[:, i])\n",
    "        recall = recall_score(actual[:, i], predicted[:, i])\n",
    "        f1 = f1_score(actual[:, i], predicted[:, i])\n",
    "        \n",
    "        metrics.append([accuracy, precision, recall, f1])\n",
    "    \n",
    "    # Create dataframe containing metrics\n",
    "    metrics = np.array(metrics)\n",
    "    metrics_df = pd.DataFrame(data = metrics, index = col_names, columns = ['Accuracy', 'Precision', 'Recall', 'F1'])\n",
    "      \n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall        F1\n",
      "request                 0.987285   0.998707  0.926237  0.961108\n",
      "offer                   0.998678   1.000000  0.717391  0.835443\n",
      "aid_related             0.986370   0.997088  0.969831  0.983271\n",
      "medical_help            0.986878   0.999216  0.832136  0.908054\n",
      "medical_products        0.993337   0.998802  0.865145  0.927182\n",
      "search_and_rescue       0.993897   0.997722  0.786355  0.879518\n",
      "security                0.995067   1.000000  0.729805  0.843800\n",
      "military                0.995830   0.998305  0.879104  0.934921\n",
      "child_alone             1.000000   0.000000  0.000000  0.000000\n",
      "water                   0.994253   0.999116  0.909823  0.952381\n",
      "food                    0.993287   1.000000  0.939142  0.968616\n",
      "shelter                 0.991150   0.999361  0.900460  0.947337\n",
      "clothing                0.996999   1.000000  0.807190  0.893309\n",
      "money                   0.995219   1.000000  0.796976  0.887019\n",
      "missing_people          0.998016   1.000000  0.821101  0.901763\n",
      "refugees                0.993744   1.000000  0.813354  0.897071\n",
      "death                   0.995016   1.000000  0.891951  0.942890\n",
      "other_aid               0.979961   0.998617  0.847086  0.916631\n",
      "infrastructure_related  0.985861   0.998020  0.785047  0.878814\n",
      "transport               0.991710   0.997319  0.822099  0.901272\n",
      "buildings               0.992218   1.000000  0.843558  0.915141\n",
      "electricity             0.997559   1.000000  0.878481  0.935310\n",
      "tools                   0.998271   1.000000  0.719008  0.836538\n",
      "hospitals               0.997711   1.000000  0.792627  0.884319\n",
      "shops                   0.998525   1.000000  0.684783  0.812903\n",
      "aid_centers             0.996948   1.000000  0.747899  0.855769\n",
      "other_infrastructure    0.989726   0.998489  0.766821  0.867454\n",
      "weather_related         0.986726   0.998284  0.954073  0.975678\n",
      "floods                  0.991964   0.999312  0.902424  0.948400\n",
      "storm                   0.994151   0.995343  0.941112  0.967468\n",
      "fire                    0.997813   1.000000  0.792271  0.884097\n",
      "earthquake              0.996236   0.997770  0.962366  0.979748\n",
      "cold                    0.997406   0.997253  0.878935  0.934363\n",
      "other_weather           0.989625   1.000000  0.804035  0.891374\n",
      "direct_report           0.983115   1.000000  0.911843  0.953889\n",
      "direct                  0.998067   0.999249  0.996007  0.997626\n",
      "news                    0.997813   0.999694  0.995937  0.997812\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics for training set\n",
    "y_train_pred = pipeline.predict(X_train)\n",
    "col_names = list(y.columns.values)\n",
    "\n",
    "print(get_eval_metrics(np.array(y_train), y_train_pred, col_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall        F1\n",
      "request                 0.879921   0.843750  0.379280  0.523319\n",
      "offer                   0.996033   0.000000  0.000000  0.000000\n",
      "aid_related             0.728410   0.764771  0.505659  0.608791\n",
      "medical_help            0.918370   0.680000  0.061483  0.112769\n",
      "medical_products        0.949954   0.783784  0.083095  0.150259\n",
      "search_and_rescue       0.974519   0.500000  0.041916  0.077348\n",
      "security                0.983064   1.000000  0.008929  0.017699\n",
      "military                0.970705   0.458333  0.057895  0.102804\n",
      "child_alone             1.000000   0.000000  0.000000  0.000000\n",
      "water                   0.945072   0.824074  0.206977  0.330855\n",
      "food                    0.915319   0.861818  0.314324  0.460641\n",
      "shelter                 0.927525   0.834437  0.218750  0.346630\n",
      "clothing                0.984590   0.400000  0.040404  0.073394\n",
      "money                   0.978181   0.333333  0.014184  0.027211\n",
      "missing_people          0.988251   1.000000  0.037500  0.072289\n",
      "refugees                0.966738   0.375000  0.013889  0.026786\n",
      "death                   0.960024   0.755102  0.128920  0.220238\n",
      "other_aid               0.863442   0.428571  0.020247  0.038668\n",
      "infrastructure_related  0.935307   0.200000  0.002375  0.004695\n",
      "transport               0.956210   0.655172  0.064189  0.116923\n",
      "buildings               0.947818   0.809524  0.047887  0.090426\n",
      "electricity             0.980317   0.750000  0.087591  0.156863\n",
      "tools                   0.994202   0.000000  0.000000  0.000000\n",
      "hospitals               0.989930   0.000000  0.000000  0.000000\n",
      "shops                   0.995728   0.000000  0.000000  0.000000\n",
      "aid_centers             0.989167   0.000000  0.000000  0.000000\n",
      "other_infrastructure    0.955600   0.333333  0.006920  0.013559\n",
      "weather_related         0.833689   0.835196  0.495580  0.622053\n",
      "floods                  0.929051   0.893204  0.168498  0.283513\n",
      "storm                   0.931340   0.807692  0.369010  0.506579\n",
      "fire                    0.988557   0.000000  0.000000  0.000000\n",
      "earthquake              0.962008   0.884444  0.668908  0.761722\n",
      "cold                    0.983064   0.636364  0.119658  0.201439\n",
      "other_weather           0.949954   0.606061  0.059701  0.108696\n",
      "direct_report           0.846964   0.810976  0.304813  0.443087\n",
      "direct                  0.941868   0.932798  0.928390  0.930588\n",
      "news                    0.953311   0.975115  0.928304  0.951134\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics for test set\n",
    "y_test_pred = pipeline.predict(X_test)\n",
    "\n",
    "eval_metrics1 = get_eval_metrics(np.array(y_test), y_test_pred, col_names)\n",
    "print(eval_metrics1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training accuracy and F1 score perform well. However when the model sees an out of sample data from the test dataset, it performs poorly. The accuracy scores are high but the F1 score is very low. This is likely due to the unbalanced nature of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "request                   0.170659\n",
       "offer                     0.004501\n",
       "aid_related               0.414251\n",
       "medical_help              0.079493\n",
       "medical_products          0.050084\n",
       "search_and_rescue         0.027617\n",
       "security                  0.017966\n",
       "military                  0.032804\n",
       "child_alone               0.000000\n",
       "water                     0.063778\n",
       "food                      0.111497\n",
       "shelter                   0.088267\n",
       "clothing                  0.015449\n",
       "money                     0.023039\n",
       "missing_people            0.011367\n",
       "refugees                  0.033377\n",
       "death                     0.045545\n",
       "other_aid                 0.131446\n",
       "infrastructure_related    0.065037\n",
       "transport                 0.045812\n",
       "buildings                 0.050847\n",
       "electricity               0.020293\n",
       "tools                     0.006065\n",
       "hospitals                 0.010795\n",
       "shops                     0.004577\n",
       "aid_centers               0.011787\n",
       "other_infrastructure      0.043904\n",
       "weather_related           0.278341\n",
       "floods                    0.082202\n",
       "storm                     0.093187\n",
       "fire                      0.010757\n",
       "earthquake                0.093645\n",
       "cold                      0.020217\n",
       "other_weather             0.052487\n",
       "direct_report             0.193584\n",
       "direct                    0.410665\n",
       "news                      0.497940\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the proportion of each column that have label == 1\n",
    "y.sum()/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calculating the proportion of each column, we can see that the data is unbalanced. Some of the categories are less than 5% of the dataset. This makes it difficult to predict. In the next session, we will apply a method to deal with the imbalanced dataset to improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define performance metric for used in the grid search \n",
    "def performance_metric(y_true, y_pred):\n",
    "    \"\"\"Calculate median F1 score for all of the output classifiers\n",
    "    \n",
    "    Args:\n",
    "    y_true: array. Array containing actual labels.\n",
    "    y_pred: array. Array containing predicted labels.\n",
    "        \n",
    "    Returns:\n",
    "    score: float. Median F1 score for all of the output classifiers\n",
    "    \"\"\"\n",
    "    f1_list = []\n",
    "    for i in range(np.shape(y_pred)[1]):\n",
    "        f1 = f1_score(np.array(y_true)[:, i], y_pred[:, i])\n",
    "        f1_list.append(f1)\n",
    "        \n",
    "    score = np.median(f1_list)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have choose use the median F1 score for the output classifiers instead of the mean. This is to prevent the uneven distribution of the F1 score taking into account extreme values and zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, total=  44.4s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   51.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, total=  46.0s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, total=  47.6s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1.0 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1.0, total=  43.9s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1.0 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1.0, total=  42.6s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1.0 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1.0, total=  44.6s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=0.75, total=  47.4s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=0.75, total=  46.1s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=0.75, total=  44.4s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=1.0 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=1.0, total=  43.8s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=1.0 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=1.0, total=  45.1s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=1.0 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=1.0, total=  44.7s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=0.75, total= 1.7min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=0.75, total= 1.7min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=0.75, total= 1.6min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=1.0 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=1.0, total= 1.5min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=1.0 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=1.0, total= 1.5min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=1.0 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=1.0, total= 1.6min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=0.75, total= 1.6min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=0.75, total= 1.9min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=0.75, total= 1.8min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=1.0 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=1.0, total= 2.1min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=1.0 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=1.0, total= 1.7min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=1.0 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=1.0, total= 1.7min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, total=  37.8s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, total=  36.1s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, total=  38.3s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1.0 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1.0, total=  34.1s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1.0 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1.0, total=  34.9s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1.0 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1.0, total=  34.1s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=0.75, total=  33.1s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=0.75, total=  36.3s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=0.75, total=  35.8s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=1.0 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=1.0, total=  34.5s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=1.0 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=1.0, total=  32.2s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=1.0 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=1.0, total=  32.2s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=0.75, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=0.75, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=0.75, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=1.0 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=1.0, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=1.0 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=1.0, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=1.0 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=1.0, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=0.75, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=0.75, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=0.75, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=1.0 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=1.0, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=1.0 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=1.0, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=1.0 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=1.0, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, total=  28.3s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, total=  29.0s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, total=  29.4s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1.0 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1.0, total=  28.9s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1.0 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1.0, total=  29.3s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1.0 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1.0, total=  29.0s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=0.75, total=  28.5s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=0.75, total=  28.2s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=0.75, total=  28.2s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=1.0 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=1.0, total=  28.3s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=1.0 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=1.0, total=  28.0s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=1.0 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__max_df=1.0, total=  29.4s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=0.75, total=  59.9s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=0.75, total= 1.0min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=0.75, total=  59.6s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=1.0 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=1.0, total=  59.7s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=1.0 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=1.0, total= 1.0min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=1.0 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__max_df=1.0, total=  59.9s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=0.75, total=  59.2s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=0.75, total=  59.2s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=0.75 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=0.75, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=1.0 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=1.0, total=  59.4s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=1.0 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=1.0, total=  59.6s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=1.0 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__max_df=1.0, total=  59.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  72 out of  72 | elapsed: 80.1min finished\n"
     ]
    }
   ],
   "source": [
    "#Modify your model to return a GridSearchCV object.\n",
    "parameters = {'vect__max_df': [0.75, 1.0],\n",
    "              'tfidf__use_idf':[True, False],\n",
    "              'clf__estimator__n_estimators':[10, 25], \n",
    "              'clf__estimator__min_samples_split':[2, 5, 10]}\n",
    "\n",
    "scorer = make_scorer(performance_metric)\n",
    "cv = GridSearchCV(pipeline, param_grid = parameters, scoring = scorer, verbose = 2)\n",
    "\n",
    "# Find best parameters\n",
    "np.random.seed(42)\n",
    "tuned_model = cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 37.4043591 ,  37.37849339,  37.58101217,  37.34998488,\n",
       "         84.4230295 ,  83.04098415,  83.26359765,  82.90529323,\n",
       "         31.84374746,  31.7137289 ,  30.90857363,  30.73453569,\n",
       "         68.86535915,  69.76065588,  67.97183466,  68.35721795,\n",
       "         29.58434391,  29.4461933 ,  28.67922926,  28.55971217,\n",
       "         63.64330308,  62.81008021,  60.54885459,  61.31373405]),\n",
       " 'std_fit_time': array([ 0.12170782,  0.13465771,  0.09260326,  0.15678535,  0.12696913,\n",
       "         0.11132765,  0.05645151,  0.0722737 ,  0.30331126,  0.26530105,\n",
       "         0.11190421,  0.16644813,  0.15746703,  0.26548985,  0.14486467,\n",
       "         0.13941127,  0.10647157,  0.07907105,  0.17734197,  0.08149233,\n",
       "         0.32487823,  0.3411261 ,  0.21035485,  0.2412044 ]),\n",
       " 'mean_score_time': array([ 4.57505337,  4.54153554,  4.55048974,  4.51354798,  6.64433058,\n",
       "         6.57727273,  6.51987433,  6.48522051,  4.57555636,  4.49807819,\n",
       "         4.47716482,  4.45687604,  6.55812836,  6.56139787,  6.5469559 ,\n",
       "         6.51376041,  4.54943649,  4.52981615,  4.52871259,  4.48921982,\n",
       "         6.57626987,  6.48400378,  6.45410117,  6.4927303 ]),\n",
       " 'std_score_time': array([ 0.04662179,  0.02137987,  0.07353856,  0.00921239,  0.02646895,\n",
       "         0.03857807,  0.02075426,  0.02249192,  0.04622209,  0.00768092,\n",
       "         0.0207547 ,  0.02404384,  0.02537122,  0.03256034,  0.03421245,\n",
       "         0.02714855,  0.03227894,  0.01574374,  0.01436402,  0.02451448,\n",
       "         0.05176092,  0.03465192,  0.01291944,  0.03368269]),\n",
       " 'param_clf__estimator__min_samples_split': masked_array(data = [2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 10 10 10 10 10 10 10 10],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_clf__estimator__n_estimators': masked_array(data = [10 10 10 10 25 25 25 25 10 10 10 10 25 25 25 25 10 10 10 10 25 25 25 25],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_tfidf__use_idf': masked_array(data = [True True False False True True False False True True False False True\n",
       "  True False False True True False False True True False False],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_vect__max_df': masked_array(data = [0.75 1.0 0.75 1.0 0.75 1.0 0.75 1.0 0.75 1.0 0.75 1.0 0.75 1.0 0.75 1.0\n",
       "  0.75 1.0 0.75 1.0 0.75 1.0 0.75 1.0],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': [{'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__max_df': 0.75},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__max_df': 1.0},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__max_df': 0.75},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__max_df': 1.0},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__max_df': 0.75},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__max_df': 1.0},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__max_df': 0.75},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__max_df': 1.0},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__max_df': 0.75},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__max_df': 1.0},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__max_df': 0.75},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__max_df': 1.0},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__max_df': 0.75},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__max_df': 1.0},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__max_df': 0.75},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__max_df': 1.0},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__max_df': 0.75},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__max_df': 1.0},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__max_df': 0.75},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__max_df': 1.0},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__max_df': 0.75},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__max_df': 1.0},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__max_df': 0.75},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__max_df': 1.0}],\n",
       " 'split0_test_score': array([ 0.10837438,  0.09090909,  0.08041958,  0.07894737,  0.11925043,\n",
       "         0.11016949,  0.07815275,  0.08571429,  0.12286689,  0.12612613,\n",
       "         0.12758621,  0.1369863 ,  0.09174312,  0.07751938,  0.08433735,\n",
       "         0.07272727,  0.12359551,  0.14713896,  0.12209302,  0.125     ,\n",
       "         0.08960573,  0.05517241,  0.05517241,  0.05830904]),\n",
       " 'split1_test_score': array([ 0.11527378,  0.09565217,  0.08743169,  0.09221902,  0.11461318,\n",
       "         0.09090909,  0.08695652,  0.08733624,  0.10650888,  0.13559322,\n",
       "         0.11976048,  0.10434783,  0.1       ,  0.10029499,  0.10951009,\n",
       "         0.07594937,  0.13496933,  0.11987382,  0.1183432 ,  0.1232493 ,\n",
       "         0.10778443,  0.09009009,  0.09174312,  0.07361963]),\n",
       " 'split2_test_score': array([ 0.11620795,  0.09195402,  0.08411215,  0.11009174,  0.12612613,\n",
       "         0.09174312,  0.10091743,  0.10958904,  0.15053763,  0.1657754 ,\n",
       "         0.13284133,  0.08256881,  0.1184669 ,  0.09230769,  0.07407407,\n",
       "         0.08333333,  0.11940299,  0.13978495,  0.12451362,  0.125     ,\n",
       "         0.11235955,  0.07287449,  0.09056604,  0.07985481]),\n",
       " 'mean_test_score': array([ 0.11328537,  0.09283843,  0.08398781,  0.09375271,  0.11999658,\n",
       "         0.09760723,  0.08867557,  0.09421319,  0.1266378 ,  0.14249825,\n",
       "         0.12672934,  0.10796764,  0.10340334,  0.09004069,  0.08930717,\n",
       "         0.07733666,  0.12598927,  0.13559924,  0.12164995,  0.12441643,\n",
       "         0.10324991,  0.07271233,  0.07916052,  0.07059449]),\n",
       " 'std_test_score': array([ 0.00349347,  0.00203484,  0.00286403,  0.0127608 ,  0.00472966,\n",
       "         0.00888938,  0.0093728 ,  0.01089251,  0.01817136,  0.01690712,\n",
       "         0.0053745 ,  0.02236281,  0.0111722 ,  0.00943527,  0.0148874 ,\n",
       "         0.00443964,  0.00657649,  0.01151773,  0.00253847,  0.00082529,\n",
       "         0.00982702,  0.01425554,  0.01696896,  0.00905239]),\n",
       " 'rank_test_score': array([ 9, 16, 20, 15,  8, 13, 19, 14,  4,  1,  3, 10, 11, 17, 18, 22,  5,\n",
       "         2,  7,  6, 12, 23, 21, 24], dtype=int32),\n",
       " 'split0_train_score': array([ 0.9117395 ,  0.90725806,  0.91262136,  0.91032149,  0.98557159,\n",
       "         0.98132935,  0.98366294,  0.98360656,  0.85539715,  0.8563163 ,\n",
       "         0.83963964,  0.8403526 ,  0.88667992,  0.90060501,  0.88446215,\n",
       "         0.88      ,  0.79697828,  0.80451128,  0.7826087 ,  0.77514793,\n",
       "         0.82516189,  0.83575884,  0.80851064,  0.82599581]),\n",
       " 'split1_train_score': array([ 0.90526316,  0.91428571,  0.91131105,  0.89964158,  0.98233996,\n",
       "         0.98169717,  0.98505114,  0.98294574,  0.8494382 ,  0.84400657,\n",
       "         0.82244556,  0.83636364,  0.88695652,  0.88937093,  0.87339744,\n",
       "         0.87964989,  0.78804855,  0.76984925,  0.75586854,  0.75208914,\n",
       "         0.80543755,  0.81086324,  0.79888268,  0.78384798]),\n",
       " 'split2_train_score': array([ 0.91819699,  0.90977042,  0.90382166,  0.91016043,  0.98101898,\n",
       "         0.98434783,  0.98337292,  0.9855585 ,  0.85433071,  0.86051081,\n",
       "         0.85148515,  0.85714286,  0.89405685,  0.88461538,  0.89312977,\n",
       "         0.88105727,  0.7875    ,  0.78702398,  0.79503106,  0.7985348 ,\n",
       "         0.82565492,  0.82565492,  0.82113821,  0.80903491]),\n",
       " 'mean_train_score': array([ 0.91173322,  0.91043807,  0.90925136,  0.90670783,  0.98297684,\n",
       "         0.98245812,  0.984029  ,  0.98403693,  0.85305535,  0.85361122,\n",
       "         0.83785678,  0.8446197 ,  0.8892311 ,  0.89153044,  0.88366312,\n",
       "         0.88023572,  0.79084228,  0.78712817,  0.7778361 ,  0.77525729,\n",
       "         0.81875145,  0.82409233,  0.80951051,  0.8062929 ]),\n",
       " 'std_train_score': array([ 0.00528022,  0.00290761,  0.00387646,  0.00499703,  0.00191238,\n",
       "         0.00134464,  0.0007324 ,  0.00110922,  0.0025945 ,  0.00700407,\n",
       "         0.0119222 ,  0.00900371,  0.00341419,  0.00670396,  0.00807548,\n",
       "         0.00059825,  0.00434459,  0.01415091,  0.01634031,  0.01896152,\n",
       "         0.0094165 ,  0.01022347,  0.00911325,  0.01731567])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get results of grid search\n",
    "tuned_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1424982491782093"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best mean test score\n",
    "np.max(tuned_model.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__min_samples_split': 5,\n",
       " 'clf__estimator__n_estimators': 10,\n",
       " 'tfidf__use_idf': True,\n",
       " 'vect__max_df': 1.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters for best mean test score\n",
    "tuned_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall        F1\n",
      "request                 0.883735   0.810544  0.431958  0.563574\n",
      "offer                   0.996033   0.000000  0.000000  0.000000\n",
      "aid_related             0.737412   0.729486  0.590727  0.652814\n",
      "medical_help            0.918370   0.593750  0.103074  0.175655\n",
      "medical_products        0.950107   0.843750  0.077364  0.141732\n",
      "search_and_rescue       0.975435   0.687500  0.065868  0.120219\n",
      "security                0.981996   0.125000  0.008929  0.016667\n",
      "military                0.970857   0.480000  0.063158  0.111628\n",
      "child_alone             1.000000   0.000000  0.000000  0.000000\n",
      "water                   0.946445   0.898990  0.206977  0.336484\n",
      "food                    0.918828   0.824561  0.374005  0.514599\n",
      "shelter                 0.930424   0.812500  0.270833  0.406250\n",
      "clothing                0.987031   0.750000  0.212121  0.330709\n",
      "money                   0.978639   0.555556  0.035461  0.066667\n",
      "missing_people          0.987946   1.000000  0.012500  0.024691\n",
      "refugees                0.968721   0.587302  0.171296  0.265233\n",
      "death                   0.960635   0.704225  0.174216  0.279330\n",
      "other_aid               0.864663   0.515152  0.038245  0.071204\n",
      "infrastructure_related  0.935459   0.416667  0.011876  0.023095\n",
      "transport               0.956973   0.659091  0.097973  0.170588\n",
      "buildings               0.951327   0.810345  0.132394  0.227603\n",
      "electricity             0.980012   0.800000  0.058394  0.108844\n",
      "tools                   0.994202   0.000000  0.000000  0.000000\n",
      "hospitals               0.990082   1.000000  0.015152  0.029851\n",
      "shops                   0.995728   0.000000  0.000000  0.000000\n",
      "aid_centers             0.989167   0.000000  0.000000  0.000000\n",
      "other_infrastructure    0.954837   0.000000  0.000000  0.000000\n",
      "weather_related         0.852304   0.820396  0.595580  0.690141\n",
      "floods                  0.944767   0.870968  0.395604  0.544081\n",
      "storm                   0.931340   0.769939  0.400958  0.527311\n",
      "fire                    0.988709   1.000000  0.013333  0.026316\n",
      "earthquake              0.959414   0.881671  0.638655  0.740741\n",
      "cold                    0.983216   0.733333  0.094017  0.166667\n",
      "other_weather           0.949344   0.565217  0.038806  0.072626\n",
      "direct_report           0.851541   0.766667  0.368984  0.498195\n",
      "direct                  0.938663   0.917823  0.937841  0.927724\n",
      "news                    0.958651   0.969019  0.945761  0.957249\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics for test set\n",
    "tuned_pred_test = tuned_model.predict(X_test)\n",
    "\n",
    "eval_metrics2 = get_eval_metrics(np.array(y_test), tuned_pred_test, col_names)\n",
    "\n",
    "print(eval_metrics2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>37.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.945789</td>\n",
       "      <td>0.566834</td>\n",
       "      <td>0.172575</td>\n",
       "      <td>0.226513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.055346</td>\n",
       "      <td>0.341899</td>\n",
       "      <td>0.248865</td>\n",
       "      <td>0.272454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.728410</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.931340</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.017699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.956210</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.059701</td>\n",
       "      <td>0.108696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.983064</td>\n",
       "      <td>0.834437</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.346630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.928390</td>\n",
       "      <td>0.951134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Accuracy  Precision     Recall         F1\n",
       "count  37.000000  37.000000  37.000000  37.000000\n",
       "mean    0.945789   0.566834   0.172575   0.226513\n",
       "std     0.055346   0.341899   0.248865   0.272454\n",
       "min     0.728410   0.000000   0.000000   0.000000\n",
       "25%     0.931340   0.333333   0.008929   0.017699\n",
       "50%     0.956210   0.680000   0.059701   0.108696\n",
       "75%     0.983064   0.834437   0.218750   0.346630\n",
       "max     1.000000   1.000000   0.928390   0.951134"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get summary stats for first model\n",
    "eval_metrics1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>37.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.947649</td>\n",
       "      <td>0.618904</td>\n",
       "      <td>0.204921</td>\n",
       "      <td>0.264554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.052897</td>\n",
       "      <td>0.327372</td>\n",
       "      <td>0.258303</td>\n",
       "      <td>0.278661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.737412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.935459</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.026316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.958651</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.094017</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.983216</td>\n",
       "      <td>0.824561</td>\n",
       "      <td>0.368984</td>\n",
       "      <td>0.498195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.945761</td>\n",
       "      <td>0.957249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Accuracy  Precision     Recall         F1\n",
       "count  37.000000  37.000000  37.000000  37.000000\n",
       "mean    0.947649   0.618904   0.204921   0.264554\n",
       "std     0.052897   0.327372   0.258303   0.278661\n",
       "min     0.737412   0.000000   0.000000   0.000000\n",
       "25%     0.935459   0.515152   0.013333   0.026316\n",
       "50%     0.958651   0.733333   0.094017   0.166667\n",
       "75%     0.983216   0.824561   0.368984   0.498195\n",
       "max     1.000000   1.000000   0.945761   0.957249"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get summary stats for tuned model\n",
    "eval_metrics2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the mean and median F1 score is higher after using grid search. However, the F1 score is very low. We can try to improve the model further using another machine learning algorithm called XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[CV] clf__estimator__learning_rate=0.05, clf__estimator__max_depth=3, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.05, clf__estimator__max_depth=3, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1, total=  12.8s\n",
      "[CV] clf__estimator__learning_rate=0.05, clf__estimator__max_depth=3, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   19.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__learning_rate=0.05, clf__estimator__max_depth=3, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1, total=  13.5s\n",
      "[CV] clf__estimator__learning_rate=0.05, clf__estimator__max_depth=3, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.05, clf__estimator__max_depth=3, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1, total=  12.2s\n",
      "[CV] clf__estimator__learning_rate=0.05, clf__estimator__max_depth=3, clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.05, clf__estimator__max_depth=3, clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__max_df=1, total=  27.2s\n",
      "[CV] clf__estimator__learning_rate=0.05, clf__estimator__max_depth=3, clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.05, clf__estimator__max_depth=3, clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__max_df=1, total=  28.8s\n",
      "[CV] clf__estimator__learning_rate=0.05, clf__estimator__max_depth=3, clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.05, clf__estimator__max_depth=3, clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__max_df=1, total=  27.2s\n",
      "[CV] clf__estimator__learning_rate=0.05, clf__estimator__max_depth=3, clf__estimator__n_estimators=100, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.05, clf__estimator__max_depth=3, clf__estimator__n_estimators=100, tfidf__use_idf=True, vect__max_df=1, total=  46.6s\n",
      "[CV] clf__estimator__learning_rate=0.05, clf__estimator__max_depth=3, clf__estimator__n_estimators=100, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.05, clf__estimator__max_depth=3, clf__estimator__n_estimators=100, tfidf__use_idf=True, vect__max_df=1, total=  46.5s\n",
      "[CV] clf__estimator__learning_rate=0.05, clf__estimator__max_depth=3, clf__estimator__n_estimators=100, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.05, clf__estimator__max_depth=3, clf__estimator__n_estimators=100, tfidf__use_idf=True, vect__max_df=1, total=  46.2s\n",
      "[CV] clf__estimator__learning_rate=0.05, clf__estimator__max_depth=4, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.05, clf__estimator__max_depth=4, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1, total=  12.2s\n",
      "[CV] clf__estimator__learning_rate=0.05, clf__estimator__max_depth=4, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.05, clf__estimator__max_depth=4, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1, total=  12.1s\n",
      "[CV] clf__estimator__learning_rate=0.05, clf__estimator__max_depth=4, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.05, clf__estimator__max_depth=4, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1, total=  12.2s\n",
      "[CV] clf__estimator__learning_rate=0.05, clf__estimator__max_depth=4, clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.05, clf__estimator__max_depth=4, clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__max_df=1, total=  27.0s\n",
      "[CV] clf__estimator__learning_rate=0.05, clf__estimator__max_depth=4, clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.05, clf__estimator__max_depth=4, clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__max_df=1, total=  29.3s\n",
      "[CV] clf__estimator__learning_rate=0.05, clf__estimator__max_depth=4, clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.05, clf__estimator__max_depth=4, clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__max_df=1, total=  26.9s\n",
      "[CV] clf__estimator__learning_rate=0.05, clf__estimator__max_depth=4, clf__estimator__n_estimators=100, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.05, clf__estimator__max_depth=4, clf__estimator__n_estimators=100, tfidf__use_idf=True, vect__max_df=1, total=  45.9s\n",
      "[CV] clf__estimator__learning_rate=0.05, clf__estimator__max_depth=4, clf__estimator__n_estimators=100, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.05, clf__estimator__max_depth=4, clf__estimator__n_estimators=100, tfidf__use_idf=True, vect__max_df=1, total=  46.2s\n",
      "[CV] clf__estimator__learning_rate=0.05, clf__estimator__max_depth=4, clf__estimator__n_estimators=100, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.05, clf__estimator__max_depth=4, clf__estimator__n_estimators=100, tfidf__use_idf=True, vect__max_df=1, total=  45.8s\n",
      "[CV] clf__estimator__learning_rate=0.1, clf__estimator__max_depth=3, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.1, clf__estimator__max_depth=3, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1, total=  12.1s\n",
      "[CV] clf__estimator__learning_rate=0.1, clf__estimator__max_depth=3, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.1, clf__estimator__max_depth=3, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1, total=  12.1s\n",
      "[CV] clf__estimator__learning_rate=0.1, clf__estimator__max_depth=3, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.1, clf__estimator__max_depth=3, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1, total=  12.1s\n",
      "[CV] clf__estimator__learning_rate=0.1, clf__estimator__max_depth=3, clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.1, clf__estimator__max_depth=3, clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__max_df=1, total=  26.8s\n",
      "[CV] clf__estimator__learning_rate=0.1, clf__estimator__max_depth=3, clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.1, clf__estimator__max_depth=3, clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__max_df=1, total=  28.7s\n",
      "[CV] clf__estimator__learning_rate=0.1, clf__estimator__max_depth=3, clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.1, clf__estimator__max_depth=3, clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__max_df=1, total=  27.2s\n",
      "[CV] clf__estimator__learning_rate=0.1, clf__estimator__max_depth=3, clf__estimator__n_estimators=100, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.1, clf__estimator__max_depth=3, clf__estimator__n_estimators=100, tfidf__use_idf=True, vect__max_df=1, total=  46.0s\n",
      "[CV] clf__estimator__learning_rate=0.1, clf__estimator__max_depth=3, clf__estimator__n_estimators=100, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.1, clf__estimator__max_depth=3, clf__estimator__n_estimators=100, tfidf__use_idf=True, vect__max_df=1, total=  45.9s\n",
      "[CV] clf__estimator__learning_rate=0.1, clf__estimator__max_depth=3, clf__estimator__n_estimators=100, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.1, clf__estimator__max_depth=3, clf__estimator__n_estimators=100, tfidf__use_idf=True, vect__max_df=1, total=  46.4s\n",
      "[CV] clf__estimator__learning_rate=0.1, clf__estimator__max_depth=4, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.1, clf__estimator__max_depth=4, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1, total=  12.1s\n",
      "[CV] clf__estimator__learning_rate=0.1, clf__estimator__max_depth=4, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.1, clf__estimator__max_depth=4, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1, total=  11.9s\n",
      "[CV] clf__estimator__learning_rate=0.1, clf__estimator__max_depth=4, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__learning_rate=0.1, clf__estimator__max_depth=4, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1, total=  11.8s\n",
      "[CV] clf__estimator__learning_rate=0.1, clf__estimator__max_depth=4, clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.1, clf__estimator__max_depth=4, clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__max_df=1, total=  27.0s\n",
      "[CV] clf__estimator__learning_rate=0.1, clf__estimator__max_depth=4, clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.1, clf__estimator__max_depth=4, clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__max_df=1, total=  28.5s\n",
      "[CV] clf__estimator__learning_rate=0.1, clf__estimator__max_depth=4, clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.1, clf__estimator__max_depth=4, clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__max_df=1, total=  28.4s\n",
      "[CV] clf__estimator__learning_rate=0.1, clf__estimator__max_depth=4, clf__estimator__n_estimators=100, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.1, clf__estimator__max_depth=4, clf__estimator__n_estimators=100, tfidf__use_idf=True, vect__max_df=1, total=  48.5s\n",
      "[CV] clf__estimator__learning_rate=0.1, clf__estimator__max_depth=4, clf__estimator__n_estimators=100, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.1, clf__estimator__max_depth=4, clf__estimator__n_estimators=100, tfidf__use_idf=True, vect__max_df=1, total=  54.0s\n",
      "[CV] clf__estimator__learning_rate=0.1, clf__estimator__max_depth=4, clf__estimator__n_estimators=100, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.1, clf__estimator__max_depth=4, clf__estimator__n_estimators=100, tfidf__use_idf=True, vect__max_df=1, total=  49.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed: 20.9min finished\n"
     ]
    }
   ],
   "source": [
    "# Try using XGBoost instead of Random Forest Classifier\n",
    "pipeline2 = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(XGBClassifier()))\n",
    "])\n",
    "\n",
    "parameters2 = {'vect__max_df': [1],\n",
    "              'tfidf__use_idf':[True],\n",
    "              'clf__estimator__n_estimators':[10,50, 100], \n",
    "              'clf__estimator__learning_rate': [0.05, 0.1],\n",
    "              'clf__estimator__max_depth': [3, 4]}\n",
    "scorer = make_scorer(performance_metric)\n",
    "cv2 = GridSearchCV(pipeline2, param_grid = parameters2, scoring = scorer, verbose = 2)\n",
    "\n",
    "# Find best parameters\n",
    "np.random.seed(42)\n",
    "tuned_model2 = cv2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([  9.96029933,  24.98443842,  43.67744033,   9.46175329,\n",
       "         25.04541747,  43.27202376,   9.3447272 ,  24.83415786,\n",
       "         43.35273353,   9.28449225,  25.23044475,  47.60735393]),\n",
       " 'mean_score_time': array([ 2.94934138,  2.83871754,  2.83432174,  2.7868286 ,  2.80853271,\n",
       "         2.77955286,  2.83252009,  2.82921894,  2.81801248,  2.75331807,\n",
       "         2.82940356,  3.05417212]),\n",
       " 'mean_test_score': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " 'mean_train_score': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " 'param_clf__estimator__learning_rate': masked_array(data = [0.05 0.05 0.05 0.05 0.05 0.05 0.1 0.1 0.1 0.1 0.1 0.1],\n",
       "              mask = [False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_clf__estimator__max_depth': masked_array(data = [3 3 3 4 4 4 3 3 3 4 4 4],\n",
       "              mask = [False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_clf__estimator__n_estimators': masked_array(data = [10 50 100 10 50 100 10 50 100 10 50 100],\n",
       "              mask = [False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_tfidf__use_idf': masked_array(data = [True True True True True True True True True True True True],\n",
       "              mask = [False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_vect__max_df': masked_array(data = [1 1 1 1 1 1 1 1 1 1 1 1],\n",
       "              mask = [False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': [{'clf__estimator__learning_rate': 0.05,\n",
       "   'clf__estimator__max_depth': 3,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__max_df': 1},\n",
       "  {'clf__estimator__learning_rate': 0.05,\n",
       "   'clf__estimator__max_depth': 3,\n",
       "   'clf__estimator__n_estimators': 50,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__max_df': 1},\n",
       "  {'clf__estimator__learning_rate': 0.05,\n",
       "   'clf__estimator__max_depth': 3,\n",
       "   'clf__estimator__n_estimators': 100,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__max_df': 1},\n",
       "  {'clf__estimator__learning_rate': 0.05,\n",
       "   'clf__estimator__max_depth': 4,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__max_df': 1},\n",
       "  {'clf__estimator__learning_rate': 0.05,\n",
       "   'clf__estimator__max_depth': 4,\n",
       "   'clf__estimator__n_estimators': 50,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__max_df': 1},\n",
       "  {'clf__estimator__learning_rate': 0.05,\n",
       "   'clf__estimator__max_depth': 4,\n",
       "   'clf__estimator__n_estimators': 100,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__max_df': 1},\n",
       "  {'clf__estimator__learning_rate': 0.1,\n",
       "   'clf__estimator__max_depth': 3,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__max_df': 1},\n",
       "  {'clf__estimator__learning_rate': 0.1,\n",
       "   'clf__estimator__max_depth': 3,\n",
       "   'clf__estimator__n_estimators': 50,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__max_df': 1},\n",
       "  {'clf__estimator__learning_rate': 0.1,\n",
       "   'clf__estimator__max_depth': 3,\n",
       "   'clf__estimator__n_estimators': 100,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__max_df': 1},\n",
       "  {'clf__estimator__learning_rate': 0.1,\n",
       "   'clf__estimator__max_depth': 4,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__max_df': 1},\n",
       "  {'clf__estimator__learning_rate': 0.1,\n",
       "   'clf__estimator__max_depth': 4,\n",
       "   'clf__estimator__n_estimators': 50,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__max_df': 1},\n",
       "  {'clf__estimator__learning_rate': 0.1,\n",
       "   'clf__estimator__max_depth': 4,\n",
       "   'clf__estimator__n_estimators': 100,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__max_df': 1}],\n",
       " 'rank_test_score': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'split0_test_score': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " 'split0_train_score': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " 'split1_test_score': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " 'split1_train_score': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " 'split2_test_score': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " 'split2_train_score': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " 'std_fit_time': array([ 0.53985516,  0.75800832,  0.12293048,  0.09360679,  1.05132109,\n",
       "         0.13781311,  0.06115433,  0.80012759,  0.23310196,  0.07294141,\n",
       "         0.67161597,  2.43200256]),\n",
       " 'std_score_time': array([ 0.10146758,  0.05256974,  0.06347917,  0.06685251,  0.05697974,\n",
       "         0.04176057,  0.04842005,  0.08316098,  0.04673306,  0.03482668,\n",
       "         0.01879025,  0.06420039]),\n",
       " 'std_test_score': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " 'std_train_score': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get results of grid search\n",
    "tuned_model2.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision  Recall        F1\n",
      "request                 0.826213   0.000000     0.0  0.000000\n",
      "offer                   0.996033   0.000000     0.0  0.000000\n",
      "aid_related             0.582087   0.000000     0.0  0.000000\n",
      "medical_help            0.915624   0.000000     0.0  0.000000\n",
      "medical_products        0.946750   0.000000     0.0  0.000000\n",
      "search_and_rescue       0.974519   0.000000     0.0  0.000000\n",
      "security                0.982911   0.000000     0.0  0.000000\n",
      "military                0.971010   0.000000     0.0  0.000000\n",
      "child_alone             1.000000   0.000000     0.0  0.000000\n",
      "water                   0.934391   0.000000     0.0  0.000000\n",
      "food                    0.884956   0.000000     0.0  0.000000\n",
      "shelter                 0.912115   0.000000     0.0  0.000000\n",
      "clothing                0.984895   0.000000     0.0  0.000000\n",
      "money                   0.978486   0.000000     0.0  0.000000\n",
      "missing_people          0.987794   0.000000     0.0  0.000000\n",
      "refugees                0.967043   0.000000     0.0  0.000000\n",
      "death                   0.956210   0.000000     0.0  0.000000\n",
      "other_aid               0.864358   0.000000     0.0  0.000000\n",
      "infrastructure_related  0.935764   0.000000     0.0  0.000000\n",
      "transport               0.954837   0.000000     0.0  0.000000\n",
      "buildings               0.945835   0.000000     0.0  0.000000\n",
      "electricity             0.979097   0.000000     0.0  0.000000\n",
      "tools                   0.994202   0.000000     0.0  0.000000\n",
      "hospitals               0.989930   0.000000     0.0  0.000000\n",
      "shops                   0.995728   0.000000     0.0  0.000000\n",
      "aid_centers             0.989167   0.000000     0.0  0.000000\n",
      "other_infrastructure    0.955905   0.000000     0.0  0.000000\n",
      "weather_related         0.723833   0.000000     0.0  0.000000\n",
      "floods                  0.916692   0.000000     0.0  0.000000\n",
      "storm                   0.904486   0.000000     0.0  0.000000\n",
      "fire                    0.988557   0.000000     0.0  0.000000\n",
      "earthquake              0.909216   0.000000     0.0  0.000000\n",
      "cold                    0.982148   0.000000     0.0  0.000000\n",
      "other_weather           0.948886   0.000000     0.0  0.000000\n",
      "direct_report           0.800275   0.000000     0.0  0.000000\n",
      "direct                  0.580256   0.000000     0.0  0.000000\n",
      "news                    0.489472   0.489472     1.0  0.657242\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics for test set\n",
    "tuned_pred_test2 = tuned_model2.predict(X_test)\n",
    "\n",
    "eval_metrics2 = get_eval_metrics(np.array(y_test), tuned_pred_test2, col_names)\n",
    "\n",
    "print(eval_metrics2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__learning_rate': 0.05,\n",
       " 'clf__estimator__max_depth': 3,\n",
       " 'clf__estimator__n_estimators': 10,\n",
       " 'tfidf__use_idf': True,\n",
       " 'vect__max_df': 1}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters for best mean test score\n",
    "tuned_model2.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XGBoost model performs poorly with only the news column having an F1 score of 0.65. The poor performance could be attributed to the imbalanced targets. We are going to use the imblearn package, which provides the ability to use simple approaches to balance the sample numbers in classes without introducing new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE  \n",
    "from imblearn.pipeline import Pipeline as imbPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imblearn package is not compatible with the sklearn Pipelines, therefore we have to use the imblearn pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "[CV] clf__estimator__learning_rate=0.05, clf__estimator__max_depth=3, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.05, clf__estimator__max_depth=3, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1, total=  12.0s\n",
      "[CV] clf__estimator__learning_rate=0.05, clf__estimator__max_depth=3, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   17.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__learning_rate=0.05, clf__estimator__max_depth=3, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1, total=  11.7s\n",
      "[CV] clf__estimator__learning_rate=0.05, clf__estimator__max_depth=3, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.05, clf__estimator__max_depth=3, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1, total=  12.8s\n",
      "[CV] clf__estimator__learning_rate=0.05, clf__estimator__max_depth=3, clf__estimator__n_estimators=100, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.05, clf__estimator__max_depth=3, clf__estimator__n_estimators=100, tfidf__use_idf=True, vect__max_df=1, total=  47.5s\n",
      "[CV] clf__estimator__learning_rate=0.05, clf__estimator__max_depth=3, clf__estimator__n_estimators=100, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.05, clf__estimator__max_depth=3, clf__estimator__n_estimators=100, tfidf__use_idf=True, vect__max_df=1, total=  46.7s\n",
      "[CV] clf__estimator__learning_rate=0.05, clf__estimator__max_depth=3, clf__estimator__n_estimators=100, tfidf__use_idf=True, vect__max_df=1 \n",
      "[CV]  clf__estimator__learning_rate=0.05, clf__estimator__max_depth=3, clf__estimator__n_estimators=100, tfidf__use_idf=True, vect__max_df=1, total=  45.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  3.5min finished\n"
     ]
    }
   ],
   "source": [
    "# Using SMOTE along with the best XGBoost parameter to deal with the imbalanced data\n",
    "sm = SMOTE(random_state=42)\n",
    "pipeline3 = imbPipeline([\n",
    "    ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(XGBClassifier()))\n",
    "])\n",
    "\n",
    "parameters3 = {'vect__max_df': [1],\n",
    "              'tfidf__use_idf':[True],\n",
    "              'clf__estimator__n_estimators':[10,100], \n",
    "              'clf__estimator__learning_rate': [0.05],\n",
    "              'clf__estimator__max_depth': [3]}\n",
    "scorer = make_scorer(performance_metric)\n",
    "cv3 = GridSearchCV(pipeline3, param_grid = parameters3, scoring = scorer, verbose = 2)\n",
    "# fitting the model\n",
    "tuned_model3 = cv3.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([  9.47085317,  43.48959867]),\n",
       " 'mean_score_time': array([ 2.76971308,  3.18081522]),\n",
       " 'mean_test_score': array([ 0.,  0.]),\n",
       " 'mean_train_score': array([ 0.,  0.]),\n",
       " 'param_clf__estimator__learning_rate': masked_array(data = [0.05 0.05],\n",
       "              mask = [False False],\n",
       "        fill_value = ?),\n",
       " 'param_clf__estimator__max_depth': masked_array(data = [3 3],\n",
       "              mask = [False False],\n",
       "        fill_value = ?),\n",
       " 'param_clf__estimator__n_estimators': masked_array(data = [10 100],\n",
       "              mask = [False False],\n",
       "        fill_value = ?),\n",
       " 'param_tfidf__use_idf': masked_array(data = [True True],\n",
       "              mask = [False False],\n",
       "        fill_value = ?),\n",
       " 'param_vect__max_df': masked_array(data = [1 1],\n",
       "              mask = [False False],\n",
       "        fill_value = ?),\n",
       " 'params': [{'clf__estimator__learning_rate': 0.05,\n",
       "   'clf__estimator__max_depth': 3,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__max_df': 1},\n",
       "  {'clf__estimator__learning_rate': 0.05,\n",
       "   'clf__estimator__max_depth': 3,\n",
       "   'clf__estimator__n_estimators': 100,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__max_df': 1}],\n",
       " 'rank_test_score': array([1, 1]),\n",
       " 'split0_test_score': array([ 0.,  0.]),\n",
       " 'split0_train_score': array([ 0.,  0.]),\n",
       " 'split1_test_score': array([ 0.,  0.]),\n",
       " 'split1_train_score': array([ 0.,  0.]),\n",
       " 'split2_test_score': array([ 0.,  0.]),\n",
       " 'split2_train_score': array([ 0.,  0.]),\n",
       " 'std_fit_time': array([ 0.39940533,  0.97161027]),\n",
       " 'std_score_time': array([ 0.05844922,  0.46991014]),\n",
       " 'std_test_score': array([ 0.,  0.]),\n",
       " 'std_train_score': array([ 0.,  0.])}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get results of grid search\n",
    "tuned_model3.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision  Recall        F1\n",
      "request                 0.826213   0.000000     0.0  0.000000\n",
      "offer                   0.996033   0.000000     0.0  0.000000\n",
      "aid_related             0.582087   0.000000     0.0  0.000000\n",
      "medical_help            0.915624   0.000000     0.0  0.000000\n",
      "medical_products        0.946750   0.000000     0.0  0.000000\n",
      "search_and_rescue       0.974519   0.000000     0.0  0.000000\n",
      "security                0.982911   0.000000     0.0  0.000000\n",
      "military                0.971010   0.000000     0.0  0.000000\n",
      "child_alone             1.000000   0.000000     0.0  0.000000\n",
      "water                   0.934391   0.000000     0.0  0.000000\n",
      "food                    0.884956   0.000000     0.0  0.000000\n",
      "shelter                 0.912115   0.000000     0.0  0.000000\n",
      "clothing                0.984895   0.000000     0.0  0.000000\n",
      "money                   0.978486   0.000000     0.0  0.000000\n",
      "missing_people          0.987794   0.000000     0.0  0.000000\n",
      "refugees                0.967043   0.000000     0.0  0.000000\n",
      "death                   0.956210   0.000000     0.0  0.000000\n",
      "other_aid               0.864358   0.000000     0.0  0.000000\n",
      "infrastructure_related  0.935764   0.000000     0.0  0.000000\n",
      "transport               0.954837   0.000000     0.0  0.000000\n",
      "buildings               0.945835   0.000000     0.0  0.000000\n",
      "electricity             0.979097   0.000000     0.0  0.000000\n",
      "tools                   0.994202   0.000000     0.0  0.000000\n",
      "hospitals               0.989930   0.000000     0.0  0.000000\n",
      "shops                   0.995728   0.000000     0.0  0.000000\n",
      "aid_centers             0.989167   0.000000     0.0  0.000000\n",
      "other_infrastructure    0.955905   0.000000     0.0  0.000000\n",
      "weather_related         0.723833   0.000000     0.0  0.000000\n",
      "floods                  0.916692   0.000000     0.0  0.000000\n",
      "storm                   0.904486   0.000000     0.0  0.000000\n",
      "fire                    0.988557   0.000000     0.0  0.000000\n",
      "earthquake              0.909216   0.000000     0.0  0.000000\n",
      "cold                    0.982148   0.000000     0.0  0.000000\n",
      "other_weather           0.948886   0.000000     0.0  0.000000\n",
      "direct_report           0.800275   0.000000     0.0  0.000000\n",
      "direct                  0.580256   0.000000     0.0  0.000000\n",
      "news                    0.489472   0.489472     1.0  0.657242\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics for test set\n",
    "tuned_pred_test3 = tuned_model3.predict(X_test)\n",
    "\n",
    "eval_metrics3 = get_eval_metrics(np.array(y_test), tuned_pred_test3, col_names)\n",
    "\n",
    "print(eval_metrics3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the imblearn package and SMOTE method made no difference in the model. Hence we settle for the tuned original model which is the Random Forest Classifier. Below are the best parameters after tuning the model:\n",
    "\n",
    "- CountVectorizer maximum df = 1\n",
    "- TfidfTransformer use_idf = True\n",
    "- Random Forest Classifier number of estimators = 10\n",
    "- Random Forest Classifier minimum samples split = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pickle the tuned model\n",
    "pickle.dump(tuned_model, open('disaster_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
